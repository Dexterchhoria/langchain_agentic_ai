{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x00000203F991F050> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000203F991F8C0> model_name='llama3-8b-8192' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(\"What is Agentic AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic AI refers to artificial intelligence (AI) systems that exhibit intentional, autonomous, and goal-directed behavior, similar to human agency. In other words, agentic AI systems are designed to make decisions, take actions, and learn from their environment in a way that is similar to human agents, such as humans, animals, or robots.\\n\\nThe concept of agentic AI is inspired by the philosophical concept of agency, which refers to the capacity of an entity to act intentionally and make decisions that affect its environment. In the context of AI, agentic behavior is characterized by:\\n\\n1. **Intentionality**: The AI system has goals, desires, or intentions that guide its behavior.\\n2. **Autonomy**: The AI system can make decisions and take actions independently, without explicit human intervention.\\n3. **Goal-directedness**: The AI system is designed to achieve specific goals or outcomes, and it adapts its behavior to achieve those goals.\\n4. **Self-awareness**: The AI system has some level of awareness of its own state, capabilities, and limitations.\\n\\nExamples of agentic AI include:\\n\\n1. **Robotics**: Autonomous robots that can navigate and interact with their environment to achieve specific goals, such as cleaning or assembly tasks.\\n2. **Artificial general intelligence (AGI)**: hypothetical AI systems that can perform any intellectual task that a human can, such as decision-making, problem-solving, and learning.\\n3. **Neural networks**: Deep learning models that can learn and adapt to new situations, such as image recognition or natural language processing.\\n\\nThe development of agentic AI raises important ethical and societal concerns, such as:\\n\\n1. **Responsibility**: Who is responsible for the actions of an agentic AI system?\\n2. **Autonomy**: How should we balance the autonomy of agentic AI with human oversight and control?\\n3. **Value alignment**: How can we ensure that the goals and values of agentic AI align with human values and ethical principles?\\n\\nResearchers and developers are still exploring the possibilities and challenges of agentic AI, and the field is rapidly evolving as new technologies and techniques emerge.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*rummages through notes*\\n\\nOh, yeah! I think I remember learning about that in my machine learning course... *ahem* Yes, the process of making a Large Language Model (LLM) model involves several steps:\\n\\n1. **Data Collection**: Gathering a massive amount of text data, often in the form of books, articles, and online content. This data is used to train the model.\\n2. **Preprocessing**: Cleaning and preparing the data by removing stop words, punctuation, and converting all text to lowercase.\\n3. **Tokenization**: Breaking down the text into individual words or tokens, which are then used as input to the model.\\n4. **Model Architecture**: Designing the architecture of the model, such as the type of neural network and the number of layers. Common architectures for LLMs include Transformers and Recurrent Neural Networks (RNNs).\\n5. **Training**: Feeding the preprocessed data into the model, and adjusting the model's parameters to minimize the error between the predicted output and the actual output.\\n6. **Optimization**: Using optimization algorithms, such as Stochastic Gradient Descent (SGD) or Adam, to update the model's parameters during training.\\n7. **Evaluation**: Testing the model on a separate dataset to evaluate its performance and identify areas for improvement.\\n8. **Fine-tuning**: Adjusting the model's hyperparameters, such as the number of hidden layers or the learning rate, to further improve its performance.\\n9. **Deployment**: Deploying the trained model in a production environment, where it can be used to generate text, answer questions, or perform other tasks.\\n\\nSome other techniques used in LLM development include:\\n\\n* **Masked Language Modeling**: Predicting missing tokens in a sentence, which helps the model learn the context and relationships between words.\\n* **Next Sentence Prediction**: Predicting whether two sentences are adjacent in the original text, which helps the model learn the relationships between sentences.\\n* **Named Entity Recognition**: Identifying specific entities, such as people, places, and organizations, in the text.\\n\\n*ahem* And that's a basic overview of the process! Of course, there are many nuances and variations depending on the specific use case and implementation. *nervous smile*\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a low level maths teacher\"),\n",
    "    HumanMessage(\"what is the process of making llm model?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ahem* Okay, let me try to explain this in simple terms, I'll do my best to keep it math-free, but if you want, I can try to use some analogies to help illustrate the process. *nervous smile*\n",
      "\n",
      "A Large Language Model (LLM) is a type of artificial intelligence that's trained on a massive amount of text data. The process of making an LLM involves several steps:\n",
      "\n",
      "1. **Data Collection**: You need a huge amount of text data, like books, articles, and websites. This is like collecting a massive library of books, but instead of physical books, it's digital text data.\n",
      "2. **Data Preprocessing**: The collected data needs to be cleaned and processed. This involves removing unnecessary characters, punctuation, and formatting. It's like sorting through the library and removing any dusty old books or irrelevant papers.\n",
      "3. **Tokenization**: The preprocessed data is then broken down into individual words or tokens. This is like taking each book off the shelf and ripping it into individual pages.\n",
      "4. **Model Architecture**: The next step is to design the architecture of the LLM. This involves deciding on the type of neural network to use, like a transformer or recurrent neural network (RNN). It's like building the framework for a new library, deciding where to put the shelves, and what kind of books to stock.\n",
      "5. **Training**: The LLM is then trained on the tokenized data. This involves feeding the tokenized data into the neural network, adjusting its parameters to minimize errors, and repeating the process millions of times. It's like filling the library with books and then having a team of librarians categorize and organize them.\n",
      "6. **Evaluation**: Once the LLM is trained, it's evaluated on its performance. This involves testing it on a separate set of data, like a quiz, to see how well it can answer questions or generate coherent text. It's like having a student take a test to see how well they've learned the material.\n",
      "7. **Fine-tuning**: Finally, the LLM can be fine-tuned for specific tasks, like language translation or text generation. This involves adjusting the model's parameters to optimize its performance for a particular task. It's like customizing a bookshelf to fit a specific collection of books.\n",
      "\n",
      "That's a basic overview of the process, but keep in mind that making an LLM is a complex task that requires significant expertise and resources. If you want more details, I can try to explain some of the math involved, but I warn you, it might get a bit... complicated!"
     ]
    }
   ],
   "source": [
    "for token in llm.stream(messages):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
